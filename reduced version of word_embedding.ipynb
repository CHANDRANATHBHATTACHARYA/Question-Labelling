{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install PyPDF2 nltk numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing preprocessing and creating word_embedding of words\n",
    "import PyPDF2\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            text += pdf_reader.pages[page_num].extract_text()\n",
    "    return text\n",
    "\n",
    "# Step 2: Preprocess the text\n",
    "def preprocess_text(text):\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    words = word_tokenize(text.lower())\n",
    "    words = [word for word in words if word.isalpha() and word not in stop_words]\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    return lemmatized_words\n",
    "\n",
    "# Step 3: Load pre-trained GloVe embeddings\n",
    "def load_glove_embeddings(glove_path):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            try:\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "            except ValueError:\n",
    "                print(f\"Skipping line with word '{word}' due to conversion error.\")\n",
    "    return embeddings_index\n",
    "\n",
    "# Step 4: Create word embeddings dictionary\n",
    "def create_word_embeddings_dictionary(words, embeddings_index):\n",
    "    word_embeddings = {word: embeddings_index[word] for word in words if word in embeddings_index}\n",
    "    return word_embeddings\n",
    "\n",
    "# Path to your PDF file\n",
    "\n",
    "pdf_path=r\"C:\\Users\\chand\\Desktop\\IIT MADRAS INTERNSHIP\\Transcripts\\ALL PDF TOGETHER\\merged.pdf\"\n",
    "\n",
    "# Path to GloVe embeddings file\n",
    "\n",
    "glove_path=r\"C:\\Users\\chand\\Desktop\\IIT MADRAS INTERNSHIP\\Transcripts\\glove.840B.300d.txt\"\n",
    "\n",
    "# Extract, preprocess and generate embeddings\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "lemmatized_words = preprocess_text(text)\n",
    "glove_embeddings = load_glove_embeddings(glove_path)\n",
    "word_embeddings = create_word_embeddings_dictionary(lemmatized_words, glove_embeddings)\n",
    "\n",
    "# Display the word embeddings dictionary\n",
    "print(word_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##performing pca\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_pca(word_embeddings):\n",
    "    words = list(word_embeddings.keys())\n",
    "    vectors = np.array(list(word_embeddings.values()))\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_vectors = pca.fit_transform(vectors)\n",
    "    \n",
    "    return words, reduced_vectors\n",
    "# Step 6: Plot words in 2D space\n",
    "def plot_words_in_2d(words, reduced_vectors):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1], color='red')\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, (reduced_vectors[i, 0], reduced_vectors[i, 1]))\n",
    "    \n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.title('Word Embeddings PCA')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA and plot the words\n",
    "words, reduced_vectors = perform_pca(word_embeddings)\n",
    "plot_words_in_2d(words, reduced_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take first 100 words and their corresponding reduced vectors\n",
    "words_100 = words[:100]\n",
    "reduced_vectors_100 = reduced_vectors[:100]\n",
    "\n",
    "# Plot the first 100 words in 2D space\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(reduced_vectors_100[:, 0], reduced_vectors_100[:, 1], color='red')\n",
    "\n",
    "for i, word in enumerate(words_100):\n",
    "    plt.annotate(word, (reduced_vectors_100[i, 0], reduced_vectors_100[i, 1]))\n",
    "\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Word Embeddings PCA (First 100 Words)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas\n",
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "question=pd.read_excel(r\"C:\\Users\\chand\\Desktop\\IIT MADRAS INTERNSHIP\\Transcripts\\students_questions.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample data (trainning data which were manually labelle for training purpose)\n",
    "data = {\n",
    "    \"Question\": [\n",
    "        \"What is the space complexity of a DataFrame?\",\n",
    "        \"What is the time complexity of sorting a DataFrame?\",\n",
    "        \"What is the time complexity of sorting a DataFrame?\",\n",
    "        \"What is the time complexity of groupby function in a DataFrame?\",\n",
    "        \"What is the time complexity of concatenate function in a DataFrame?\",\n",
    "        \"Are there any vectorized operations in Pandas DataFrame columns?\",\n",
    "        \"Pandas\",\n",
    "        \"Comparing two one-dimensional arrays\",\n",
    "        \"How to handle multiple files concurrently?\",\n",
    "        \"Fastest way to compare two arrays?\",\n",
    "        \"Using threads for handling multiple files...using Python?\",\n",
    "        \"Comparing two numpy arrays\",\n",
    "        \"How much time does it take to read a JSON file?\",\n",
    "        \"Efficient methods for creating directories and processing JSON files?\",\n",
    "        \"Dictionary storage vs list storage: which is faster in access time?\",\n",
    "        \"Time complexity in adding new column in a DataFrame?\",\n",
    "        \"How can I improve performance when using groupby in Pandas DataFrame?\",\n",
    "        \"What are the recommended practices for handling exceptions in Python?\",\n",
    "        \"Optimizing File Handling and Directory Creation in Python\",\n",
    "        \"How is searching in dictionaries done?\",\n",
    "        \"Time complexity of searching in a dictionary\",\n",
    "        \"How can Python handle processing multiple files concurrently?\",\n",
    "        \"What are the advantages of using threads for concurrent file processing?\",\n",
    "        \"What are some best practices for optimizing performance in Python?\"\n",
    "    ],\n",
    "    \"Solo Class\": [\n",
    "        \"Multistructural\",\n",
    "        \"Multistructural\",\n",
    "        \"Multistructural\",\n",
    "        \"Multistructural\",\n",
    "        \"Multistructural\",\n",
    "        \"Relational\",\n",
    "        \"Unistructural\",\n",
    "        \"Unistructural\",\n",
    "        \"Unistructural\",\n",
    "        \"Unistructural\",\n",
    "        \"Multistructural\",\n",
    "        \"Unistructural\",\n",
    "        \"Unistructural\",\n",
    "        \"Multistructural\",\n",
    "        \"Relational\",\n",
    "        \"Multistructural\",\n",
    "        \"Relational\",\n",
    "        \"Extended Abstract\",\n",
    "        \"Relational\",\n",
    "        \"Multistructural\",\n",
    "        \"Multistructural\",\n",
    "        \"Multistructural\",\n",
    "        \"Multistructural\",\n",
    "        \"Extended Abstract\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "question_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Function to get the average embedding for a question\n",
    "def get_question_embedding(question, word_embeddings):\n",
    "    words = word_tokenize(question.lower())\n",
    "    valid_embeddings = [word_embeddings[word] for word in words if word in word_embeddings]\n",
    "    if valid_embeddings:\n",
    "        return np.mean(valid_embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(next(iter(word_embeddings.values())).shape)\n",
    "\n",
    "# Create embeddings for each question\n",
    "question_df['Embeddings'] = question_df['Question'].apply(lambda x: get_question_embedding(x, word_embeddings))\n",
    "X = np.vstack(question_df['Embeddings'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rake_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import nltk\n",
    "from rake_nltk import Rake\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure necessary NLTK data is downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Function to fetch questions from Stack Exchange API\n",
    "def fetch_questions_from_stack_exchange(tags, pagesize=100, max_pages=20):\n",
    "    questions = []\n",
    "    base_url = 'https://api.stackexchange.com/2.3/questions'\n",
    "    for page in range(1, max_pages + 1):\n",
    "        params = {\n",
    "            'order': 'desc',\n",
    "            'sort': 'creation',\n",
    "            'tagged': tags,\n",
    "            'site': 'stackoverflow',\n",
    "            'pagesize': pagesize,\n",
    "            'page': page\n",
    "        }\n",
    "        response = requests.get(base_url, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: Unable to fetch data (status code {response.status_code})\")\n",
    "            continue\n",
    "        \n",
    "        data = response.json()\n",
    "        for item in data['items']:\n",
    "            questions.append(item['title'])\n",
    "    \n",
    "    return questions\n",
    "\n",
    "# Tags related to PDSA\n",
    "tags = 'python;data-structures;algorithms'\n",
    "\n",
    "# Fetch questions for the specified tags\n",
    "all_questions = fetch_questions_from_stack_exchange(tags, max_pages=20)\n",
    "\n",
    "# Create a DataFrame with the fetched questions\n",
    "questions_df = pd.DataFrame(all_questions, columns=['Question'])\n",
    "\n",
    "# Labeling function\n",
    "def label_question(question):\n",
    "    if any(keyword in question.lower() for keyword in ['complexity', 'performance']):\n",
    "        return 'Multistructural'\n",
    "    elif any(keyword in question.lower() for keyword in ['compare', 'difference']):\n",
    "        return 'Relational'\n",
    "    elif any(keyword in question.lower() for keyword in ['how', 'what', 'why']):\n",
    "        return 'Unistructural'\n",
    "    elif any(keyword in question.lower() for keyword in ['best practices', 'optimization']):\n",
    "        return 'Extended Abstract'\n",
    "    else:\n",
    "        return 'Prestructural'\n",
    "\n",
    "\n",
    "\n",
    "rake = Rake()\n",
    "\n",
    "\n",
    "def extract_keywords_with_rake(text):\n",
    "    rake.extract_keywords_from_text(text)\n",
    "    keywords = rake.get_ranked_phrases()[:5]  # Extract top 5 keywords\n",
    "    # Join keywords into a single string separated by commas\n",
    "    clean_keywords = ', '.join(keywords)\n",
    "    return clean_keywords\n",
    "\n",
    "# Extract keywords for each question and store them in the ImportantKeywords column\n",
    "#questions_df['ImportantKeywords'] = extract_keywords_with_rake(questions_df['Question'])\n",
    "questions_df['ImportantKeywords']=questions_df['Question'].apply(extract_keywords_with_rake)\n",
    "\n",
    "# Print the DataFrame with ImportantKeywords\n",
    "#print(questions_df.head())\n",
    "\n",
    "# Assuming question_df is your existing DataFrame\n",
    "combined_df = pd.concat([question_df, questions_df], ignore_index=True)\n",
    "\n",
    "# Print the combined DataFrame\n",
    "#print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"Question\": [\n",
    "        \"What is the space complexity of a DataFrame?\",\n",
    "        \"What is the time complexity of sorting a DataFrame?\",\n",
    "        \"What is the time complexity of sorting a DataFrame?\",\n",
    "        \"What is the time complexity of groupby function in a DataFrame?\",\n",
    "        \"What is the time complexity of concatenate function in a DataFrame?\",\n",
    "        \"Are there any vectorized operations in Pandas DataFrame columns?\",\n",
    "        \"Pandas\",\n",
    "        \"Comparing two one-dimensional arrays\",\n",
    "        \"How to handle multiple files concurrently?\",\n",
    "        \"Fastest way to compare two arrays?\",\n",
    "        \"Using threads for handling multiple files...using Python?\",\n",
    "        \"Comparing two numpy arrays\",\n",
    "        \"How much time does it take to read a JSON file?\",\n",
    "        \"Efficient methods for creating directories and processing JSON files?\",\n",
    "        \"Dictionary storage vs list storage: which is faster in access time?\",\n",
    "        \"Time complexity in adding new column in a DataFrame?\",\n",
    "        \"How can I improve performance when using groupby in Pandas DataFrame?\",\n",
    "        \"What are the recommended practices for handling exceptions in Python?\",\n",
    "        \"Optimizing File Handling and Directory Creation in Python\",\n",
    "        \"How is searching in dictionaries done?\",\n",
    "        \"Time complexity of searching in a dictionary\",\n",
    "        \"How can Python handle processing multiple files concurrently?\",\n",
    "        \"What are the advantages of using threads for concurrent file processing?\",\n",
    "        \"What are some best practices for optimizing performance in Python?\",\n",
    "        \"Efficient Merge sort implementation in python\",\n",
    "        \"Does a recursive segment tree require more space than an iterative one?\",\n",
    "        \"Implementing a FIFO allocation between two lists in Python\",\n",
    "        \"Leetcode - Combination sum problem in Python\",\n",
    "        \"Maze solving algorithm gets stuck and keeps running in \\\"circles\\\"\",\n",
    "        \"Branch and bound algorithm to find the shortest path with constraints\",\n",
    "        \"Find words with only one letter difference from a list of words\",\n",
    "        \"Algorithm for compound fractions\",\n",
    "        \"Rotate an array in-place\",\n",
    "        \"Why I am getting MLE(memory limit exceeded) in hackerearth platform\",\n",
    "        \"Hire K workers test case fail\",\n",
    "        \"Strategies for Enhancing Algorithm Efficiency?\",\n",
    "        \"Minimum window substring not sliding correctly\",\n",
    "        \"Calculate the all the possible difference between elements of a list in an optimized way\",\n",
    "        \"How can I efficiently sort a large dataset in Python?\",\n",
    "        \"Time limit exceeded on the code: Check If a String Contains All Binary Codes of Size K\",\n",
    "        \"Implementing a max heap in python using heapq for a custom class object with custom comparators in Python?\",\n",
    "        \"Edge case of binary subarray with sum\",\n",
    "        \"Number of hits in Fibonacci using dynamic programming\",\n",
    "        \"How does this seemingly random bit of string solve the TwoSum problem?\",\n",
    "        \"Recreate a O(n*k) algorithm to Θ(n)\",\n",
    "        \"Can I further speed up query to find all objects within a range in python\",\n",
    "        \"An algorithm to find the shortest path based on 2 criteria\",\n",
    "        \"Flatten Binary Tree explanation\",\n",
    "        \"A probabilistic data structure based on flipping bits with probability `1/2^x` for counting\",\n",
    "        \"Binary Tree Max Path Sum: Faulty Logic\",\n",
    "        \"How to increment each sublist in a given list by a step of one?\",\n",
    "        \"Splitting binary tree code does not work on some cases\",\n",
    "        \"Why does interpolation search calculate the position same as linear search\",\n",
    "        \"How to split a tree in branches based on depth\",\n",
    "        \"Using clear() vs copy() in a hashset\",\n",
    "        \"LeetCode 2467: Most Profitable Path in a Tree\",\n",
    "        \"Generate list of pattern contained in dictionary values Python\",\n",
    "        \"Depth First Search Logic Recursive Function\",\n",
    "        \"Solution of maze\",\n",
    "        \"Create a hierarchy data structure from list of data\",\n",
    "        \"What is a more optimal algorithm for this problem?\",\n",
    "        \"Using prefix sum to calculate scores of 2 robots in a 2d array\",\n",
    "        \"Optimizing a geometrical algorithm [ points inside a region inside a circle ]\",\n",
    "        \"How to use mergesort on a list of tuples in Python?\",\n",
    "        \"Number of permutation operations to return to original array\",\n",
    "        \"Make Pairs Of Users using python\",\n",
    "        \"Does a solution better than O(N^2) exist Python\",\n",
    "        \"Extract values from json data with condition and add them in as new columns\",\n",
    "        \"Finding the minimum absolute difference between elements minimum of k distance apart\",\n",
    "        \"Why am I getting error in this K-Nearest Neighbour algorithm?\",\n",
    "        \"Efficiently Identifying non-overlapping Intervals in a list of types in. python\",\n",
    "        \"How to perform mathematical operations on the value of an existing dictionary?\",\n",
    "        \"How to add frequency of different letters for different positions across words in a list?\",\n",
    "        \"Python Dictionary Search is taking long time\",\n",
    "        \"How to generate a unique set of 3 digit numbers from an array\",\n",
    "        \"How to retrieve specific matches from Trie search results in Python?\",\n",
    "        \"I am trying to output data in my terminal upon a tkinter button click, I am getting this error but I am not sure where I am going wrong\",\n",
    "        \"How could I improve my data structure for superpixel coordinates storage in image segmentation algorithms?\",\n",
    "        \"Submission and custom input on GeeksForGeeks gives different judge result on same test case\",\n",
    "        \"someone help me understand what is constant factor of a computer and what is mean by \\\"faster computer have low constant factor\\\"\"\n",
    "    ],\n",
    "    \"Label\": [\n",
    "        \"Multistructural\",\n",
    "        \"Multistructural\",\n",
    "        \"Multistructural\",\n",
    "        \"Multistructural\",\n",
    "        \"Multistructural\",\n",
    "        \"Multistructural\",\n",
    "        \"Prestructural\",\n",
    "        \"Relational\",\n",
    "        \"Relational\",\n",
    "        \"Multistructural\",\n",
    "        \"Relational\",\n",
    "        \"Multistructural\",\n",
    "        \"Unistructural\",\n",
    "        \"Multistructural\",\n",
    "        \"Relational\",\n",
    "        \"Multistructural\",\n",
    "        \"Relational\",\n",
    "        \"Relational\",\n",
    "        \"Relational\",\n",
    "        \"Relational\",\n",
    "        \"Multistructural\",\n",
    "        \"Relational\",\n",
    "        \"Relational\",\n",
    "        \"Relational\",\n",
    "        \"Multistructural\",\n",
    "        \"Relational\",\n",
    "        \"Multistructural\",\n",
    "        \"Relational\",\n",
    "        \"Prestructural\",\n",
    "        \"Relational\",\n",
    "        \"Unistructural\",\n",
    "        \"Unistructural\",\n",
    "        \"Unistructural\",\n",
    "        \"Prestructural\",\n",
    "        \"Unistructural\",\n",
    "        \"Relational\",\n",
    "        \"Multistructural\",\n",
    "        \"Relational\",\n",
    "        \"Relational\",\n",
    "        \"Unistructural\",\n",
    "        \"Multistructural\",\n",
    "        \"Multistructural\",\n",
    "        \"Multistructural\",\n",
    "        \"Unistructural\",\n",
    "        \"Extended Abstract\",\n",
    "        \"Multistructural\",\n",
    "        \"Relational\",\n",
    "        \"Unistructural\",\n",
    "        \"Extended Abstract\",\n",
    "        \"Prestructural\",\n",
    "        \"Unistructural\",\n",
    "        \"Unistructural\",\n",
    "        \"Multistructural\",\n",
    "        \"Unistructural\",\n",
    "        \"Prestructural\",\n",
    "        \"Unistructural\",\n",
    "        \"Relational\",\n",
    "        \"Unistructural\",\n",
    "        \"Unistructural\",\n",
    "        \"Relational\",\n",
    "        \"Multistructural\",\n",
    "        \"Unistructural\",\n",
    "        \"Unistructural\",\n",
    "        \"Multistructural\",\n",
    "        \"Unistructural\",\n",
    "        \"Multistructural\",\n",
    "        \"Extended Abstract\",\n",
    "        \"Unistructural\",\n",
    "        \"Multistructural\",\n",
    "        \"Unistructural\",\n",
    "        \"Multistructural\",\n",
    "        \"Unistructural\",\n",
    "        \"Unistructural\",\n",
    "        \"Prestructural\",\n",
    "        \"Unistructural\",\n",
    "        \"Unistructural\",\n",
    "        \"Prestructural\",\n",
    "        \"Extended Abstract\",\n",
    "        \"Prestructural\",\n",
    "        \"Unistructural\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display DataFrame\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import nltk\n",
    "from rake_nltk import Rake\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Ensure the NLTK stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess text to remove stop words\n",
    "def preprocess_text(text):\n",
    "    words = text.split()\n",
    "    return ' '.join([word for word in words if word.lower() not in stop_words])\n",
    "\n",
    "# Preprocess the questions\n",
    "df['PreprocessedQuestion'] = df['Question'].apply(preprocess_text)\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['PreprocessedQuestion'])\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "rake = Rake()\n",
    "\n",
    "# Extract keywords using RAKE and clean the output\n",
    "def extract_keywords_with_rake(text):\n",
    "    rake.extract_keywords_from_text(text)\n",
    "    keywords = rake.get_ranked_phrases()[:5]  # Extract top 5 keywords\n",
    "    # Split concatenated keywords\n",
    "    clean_keywords = []\n",
    "    for kw in keywords:\n",
    "        clean_keywords.extend(kw.split())\n",
    "    return clean_keywords\n",
    "\n",
    "\n",
    "#df['ImportantKeywords'] = [extract_keywords(tfidf_matrix[row], feature_names) for row in range(tfidf_matrix.shape[0])]\n",
    "\n",
    "df['ImportantKeywords'] = df['PreprocessedQuestion'].apply(extract_keywords_with_rake)\n",
    "\n",
    "\n",
    "# Calculate average distance between keywords\n",
    "def calculate_average_distance(keywords, embeddings):\n",
    "    vectors = [embeddings[word] for word in keywords if word in embeddings]\n",
    "    if len(vectors) < 2:\n",
    "        return 0  # Not enough keywords to calculate distance\n",
    "    \n",
    "    distances = euclidean_distances(vectors)\n",
    "    total_distance = 0\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(len(vectors)):\n",
    "        for j in range(i + 1, len(vectors)):\n",
    "            total_distance += distances[i, j]\n",
    "            count += 1\n",
    "    \n",
    "    if count == 0:\n",
    "        return 0  # To avoid division by zero if no pairs are found\n",
    "    \n",
    "    return total_distance / count\n",
    "\n",
    "df['AvgDistance'] = df['ImportantKeywords'].apply(lambda x: calculate_average_distance(x, word_embeddings))\n",
    "\n",
    "# Calculate average distance for each category\n",
    "category_avg_distances = df.groupby('Label')['AvgDistance'].mean().to_dict()\n",
    "\n",
    "#import ace_tools as tools; tools.display_dataframe_to_user(name=\"Questions with Average Distances\", dataframe=df)\n",
    "\n",
    "print(\"Category Average Distances:\\n\", category_avg_distances)\n",
    "#print(df[['Question', 'Label', 'ImportantKeywords', 'AvgDistance']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions=combined_df['Question'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_question_df=questions_df['Question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from rake_nltk import Rake\n",
    "\n",
    "# Ensure the NLTK stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Preprocess text to remove stop words\n",
    "def preprocess_text(text):\n",
    "    words = text.split()\n",
    "    return ' '.join([word for word in words if word.lower() not in stop_words])\n",
    "\n",
    "# Preprocess the new questions\n",
    "new_df = pd.DataFrame(all_questions, columns=['Question'])\n",
    "new_df['PreprocessedQuestion'] = new_df['Question'].apply(preprocess_text)\n",
    "\n",
    "# Initialize RAKE keyword extractor\n",
    "rake = Rake()\n",
    "\n",
    "# Extract keywords using RAKE and clean the output\n",
    "def extract_keywords_with_rake(text):\n",
    "    rake.extract_keywords_from_text(text)\n",
    "    keywords = rake.get_ranked_phrases()[:5]  # Extract top 5 keywords\n",
    "    # Split concatenated keywords\n",
    "    clean_keywords = []\n",
    "    for kw in keywords:\n",
    "        clean_keywords.extend(kw.split())\n",
    "    return clean_keywords\n",
    "\n",
    "new_df['ImportantKeywords'] = new_df['PreprocessedQuestion'].apply(extract_keywords_with_rake)\n",
    "\n",
    "# Calculate average distance between keywords\n",
    "def calculate_average_distance(keywords, embeddings):\n",
    "    vectors = [embeddings[word] for word in keywords if word in embeddings]\n",
    "    if len(vectors) < 2:\n",
    "        return 0  # Not enough keywords to calculate distance\n",
    "    \n",
    "    distances = euclidean_distances(vectors)\n",
    "    total_distance = 0\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(len(vectors)):\n",
    "        for j in range(i + 1, len(vectors)):\n",
    "            total_distance += distances[i, j]\n",
    "            count += 1\n",
    "    \n",
    "    if count == 0:\n",
    "        return 0  # To avoid division by zero if no pairs are found\n",
    "    \n",
    "    return total_distance / count\n",
    "\n",
    "new_df['AvgDistance'] = new_df['ImportantKeywords'].apply(lambda x: calculate_average_distance(x, word_embeddings))\n",
    "\n",
    "# Thresholds from previously computed data\n",
    "thresholds ={'Extended Abstract': 7.346035873889923, 'Multistructural': 7.234015334291118, 'Prestructural': 6.328144119996013, 'Relational': 7.291196525891623, 'Unistructural': 7.088332765060241}\n",
    "\n",
    "# Function to classify new questions\n",
    "def classify_question(avg_distance, thresholds):\n",
    "    closest_category = min(thresholds.keys(), key=lambda k: abs(thresholds[k] - avg_distance))\n",
    "    return closest_category\n",
    "\n",
    "# Classify each new question\n",
    "new_df['PredictedClass'] = new_df['AvgDistance'].apply(lambda x: classify_question(x, thresholds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##some EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1=new_df['PredictedClass'].tolist()\n",
    "new_dict={}\n",
    "for element in l1:\n",
    "    if element not in new_dict:\n",
    "        new_dict[element]=1\n",
    "    else:\n",
    "        new_dict[element]=new_dict[element]+1\n",
    "new_dict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data for the bar graph\n",
    "categories = ['Multistructural', 'Extended Abstract', 'Prestructural','Relational','Unistructural']\n",
    "counts = [34,487,136,27,142]\n",
    "\n",
    "# Plotting the bar graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(categories, counts, color=['blue', 'green', 'red','yellow','purple'])\n",
    "plt.xlabel('Question Categories')\n",
    "plt.ylabel('Number of Questions')\n",
    "plt.title('Number of Questions by Category')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "# Create a list of all keywords\n",
    "all_keywords = [keyword for sublist in new_df['ImportantKeywords'] for keyword in sublist]\n",
    "\n",
    "# Count the frequency of each keyword\n",
    "keyword_counts = Counter(all_keywords)\n",
    "\n",
    "# Get the top 50 keywords\n",
    "top_50_keywords = dict(keyword_counts.most_common(50))\n",
    "\n",
    "# Plot the top 50 keywords\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(top_50_keywords.keys(), top_50_keywords.values())\n",
    "plt.xlabel('Keywords')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 50 Keywords by Frequency')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_questions=pd.read_csv(r\"C:\\Users\\chand\\Desktop\\IIT MADRAS INTERNSHIP\\Transcripts\\QueryResults.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_questions= new_questions.drop(columns=['Post Link','User Link','CreationDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list=new_questions['Title'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_questions['PreprocessedQuestion'] = new_questions['Title'].apply(preprocess_text)\n",
    "new_questions['ImportantKeywords'] = new_questions['PreprocessedQuestion'].apply(extract_keywords_with_rake)\n",
    "new_questions['AvgDistance'] = new_questions['ImportantKeywords'].apply(lambda x: calculate_average_distance(x, word_embeddings))\n",
    "new_questions['PredictedClass'] = new_questions['AvgDistance'].apply(lambda x: classify_question(x, thresholds))\n",
    "new_questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2=new_questions['PredictedClass'].tolist()\n",
    "new_dict1={}\n",
    "for element in l2:\n",
    "    if element not in new_dict1:\n",
    "        new_dict1[element]=1\n",
    "    else:\n",
    "        new_dict1[element]=new_dict1[element]+1\n",
    "new_dict1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data for the bar graph\n",
    "categories =list(new_dict1.keys())\n",
    "counts =list(new_dict1.values())\n",
    "\n",
    "# Plotting the bar graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(categories, counts, color=['blue', 'green', 'red','yellow','purple'])\n",
    "plt.xlabel('Question Categories')\n",
    "plt.ylabel('Number of Questions')\n",
    "plt.title('Number of Questions by Category')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "# Create a list of all keywords\n",
    "all_keywords = [keyword for sublist in new_questions['ImportantKeywords'] for keyword in sublist]\n",
    "\n",
    "# Count the frequency of each keyword\n",
    "keyword_counts = Counter(all_keywords)\n",
    "\n",
    "# Get the top 50 keywords\n",
    "top_50_keywords = dict(keyword_counts.most_common(50))\n",
    "\n",
    "# Plot the top 50 keywords\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.bar(top_50_keywords.keys(), top_50_keywords.values())\n",
    "plt.xlabel('Keywords')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 50 Keywords by Frequency')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3=new_questions['AnswerCount'].tolist()\n",
    "new_dict2={}\n",
    "for element in l3:\n",
    "    if element not in new_dict2:\n",
    "        new_dict2[element]=1\n",
    "    else:\n",
    "        new_dict2[element]=new_dict2[element]+1\n",
    "new_dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = list(new_dict2.keys())\n",
    "values = list(new_dict2.values())\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.bar(categories, values)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Number of Answers')\n",
    "plt.ylabel('Nunber of Count')\n",
    "plt.title('Number of count for each number of Answers')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l4=new_questions['ViewCount'].tolist()\n",
    "l5=new_questions['PredictedClass'].tolist()\n",
    "new_set=set(l5)\n",
    "l6=list(new_set)\n",
    "dict4=dict()\n",
    "for element in l6:\n",
    "    dict4[element]=0\n",
    "n=len(l4)\n",
    "for i in range(n):\n",
    "    dict4[l5[i]]=dict4[l5[i]]+l4[i]\n",
    "dict4        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = list(dict4.keys())\n",
    "values = list(dict4.values())\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.bar(categories, values)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Question Category')\n",
    "plt.ylabel('Number of views')\n",
    "plt.title('Number of views of each type of questions')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l7=new_questions['AnswerCount'].tolist()\n",
    "l8=new_questions['PredictedClass'].tolist()\n",
    "new_set=set(l8)\n",
    "l9=list(new_set)\n",
    "dict5=dict()\n",
    "for element in l9:\n",
    "    dict5[element]=0\n",
    "n=len(l8)\n",
    "for i in range(n):\n",
    "    dict5[l8[i]]=dict5[l8[i]]+l7[i]\n",
    "dict5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data for the bar graph\n",
    "categories =list(dict5.keys())\n",
    "counts =list(dict5.values())\n",
    "\n",
    "# Plotting the bar graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(categories, counts, color=['blue', 'green', 'red','yellow','purple'])\n",
    "plt.xlabel('Question Categories')\n",
    "plt.ylabel('Number of Answers')\n",
    "plt.title('Number of  total Answers of each type of questions')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
